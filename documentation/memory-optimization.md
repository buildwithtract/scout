# Memory Optimization for Data Fetchers

This document outlines the memory optimization strategies implemented in the data fetchers, particularly for the flood risk zones fetcher.

## Key Optimizations Implemented

### 1. Eliminated Individual Database Queries ‚≠ê **Major Improvement**

- **Before**: Each feature triggered a separate database query to check if it exists
- **After**: Pre-load all existing references into an in-memory cache
- **Memory Impact**: Eliminates thousands of individual database calls that were causing memory buildup
- **Performance**: Dramatically faster processing and consistent memory usage

### 2. Batch Processing

- **Before**: Individual database operations for each feature
- **After**: Batching operations in groups of 1000 items (increased from 100)
- **Memory Impact**: Reduces database connection overhead and enables transaction-level optimizations

### 3. Aggressive Memory-Aware Processing

- **Feature**: `MemoryAwareBatchProcessor` class monitors heap usage every 100 items
- **Behavior**: Triggers batch processing when memory usage exceeds 30MB threshold (lowered from 150MB)
- **Benefit**: Prevents memory buildup during large dataset processing

### 4. Geometry Precision Reduction

- **Before**: Full precision coordinates stored as JSON strings
- **After**: Coordinates rounded to 6 decimal places (~1 meter precision)
- **Memory Impact**: Can reduce geometry string size by 20-40%

### 5. Optimized JSON Stringification

- **Feature**: Custom `stringifyGeometry()` function
- **Improvements**:
  - Handles different geometry types properly
  - Reduces floating-point precision issues
  - More memory-efficient than standard JSON.stringify

### 6. Explicit Memory Cleanup

- **Garbage Collection**: Multiple forced GC calls after each batch
- **Reference Clearing**: Aggressive cleanup of feature objects
- **Cache Management**: Cache cleared after processing to free memory

### 7. Smart Monitoring

- **Memory Logging**: Only when actually needed (30MB+ threshold)
- **Milestone Reporting**: Progress updates every 25,000 features
- **Debug Information**: Memory warnings when batch processing doesn't reduce usage

## Running with Optimized Memory Usage

### Recommended Node.js Flags

```bash
# Enable garbage collection control
bun run --expose-gc bin/fetch.ts

# Or with Node.js
node --expose-gc --max-old-space-size=2048 bin/fetch.ts
```

### Environment Variables

```bash
# Enable development warnings
NODE_ENV=development

# Increase memory limit if needed
NODE_OPTIONS="--max-old-space-size=4096"
```

### Expected Log Output (With Optimizations)

```bash
Starting fetch for flood-risk-zone with memory optimization
Loading existing references into memory cache...
Cached 15000 existing references
Saving features total 1000 inserted 1000 updated 0
Saving features total 10000 inserted 8000 updated 2000
[FloodRiskZones-25000] Memory - Heap: 45MB/60MB, RSS: 120MB, External: 5MB
Saving features total 50000 inserted 40000 updated 10000
[FloodRiskZones-50000] Memory - Heap: 35MB/45MB, RSS: 110MB, External: 3MB
Completed fetch for flood-risk-zone - processed 75000 features
```

**Memory warnings only if something goes wrong:**

```bash
Memory threshold exceeded (30MB), flushing batch of 850 items
Memory still high after batch processing: 45MB
```

## Performance Improvements

### Memory Usage Reduction

- **Typical reduction**: 80-90% lower peak memory usage (vs original unoptimized version)
- **Consistent memory**: Should stay well under 100MB during processing (vs 3+ GB before)
- **Geometry storage**: 20-40% smaller JSON strings
- **Database efficiency**: Eliminates thousands of individual queries

### Processing Speed

- **Database operations**: Much faster due to cache + transaction batching
- **I/O overhead**: Dramatically reduced (no individual existence queries)
- **GC pressure**: Lower due to aggressive memory management
- **Cache lookups**: O(1) HashMap lookups vs database queries

### Scalability

- **Large datasets**: Should handle any size dataset within reasonable memory limits
- **Memory thresholds**: Adaptive processing based on 30MB threshold
- **Early warnings**: Memory monitoring every 100 items for quick response

## Additional Optimization Suggestions

### 1. Database Connection Pooling

```typescript
// Consider using connection pooling for better resource management
const pool = new Pool({
  max: 20,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000
})
```

### 2. Streaming Database Operations

For very large datasets, consider:

- Using COPY commands for bulk inserts
- Streaming database connections
- Prepared statements for repeated operations

### 3. System-Level Optimizations

```bash
# Increase system file descriptor limits
ulimit -n 65536

# Monitor system memory
watch -n 5 'free -h && ps aux --sort=-%mem | head -20'
```

### 4. Alternative Processing Strategies

For extremely large datasets:

- **Chunked processing**: Split datasets into smaller files
- **Parallel processing**: Use worker threads for CPU-intensive operations
- **Disk-based caching**: Use temporary files for intermediate results

## Monitoring and Debugging

### Memory Monitoring Commands

```bash
# Monitor Node.js process memory
ps -o pid,ppid,%mem,rss,vsz,comm -p $(pgrep -f "fetch.ts")

# Detailed memory analysis
node --expose-gc --inspect bin/fetch.ts
```

### Log Analysis

Look for these log patterns:

**Normal operation (clean, minimal logging):**

```
Starting fetch for flood-risk-zone with memory optimization
Saving features total 10000 inserted 8000 updated 2000
[FloodRiskZones-10000] Memory usage - Heap: 120MB/180MB, RSS: 200MB
Completed fetch for flood-risk-zone - processed 25000 features
```

**High memory usage warnings (only when needed):**

```
[FloodRiskZones-15000] Memory usage - Heap: 180MB/220MB, RSS: 280MB
```

**Error conditions:**

```
Feature missing reference property, skipping
Failed to stringify geometry for reference: ABC123
```

## Troubleshooting Common Issues

### Out of Memory Errors

1. **Increase heap size**: `--max-old-space-size=4096`
2. **Reduce batch size**: Lower the batch size in the constructor
3. **Lower memory threshold**: Reduce the memory threshold trigger
4. **Enable GC**: Run with `--expose-gc` flag

### Slow Processing

1. **Check batch size**: May be too small for your dataset
2. **Database connections**: Ensure connection pooling is optimized
3. **Memory monitoring overhead**: Reduce logging frequency

### High Memory Usage Warnings

1. **Expected behavior**: During large dataset processing
2. **Monitor trends**: Ensure memory doesn't continuously grow
3. **Adjust thresholds**: Based on your system's available memory

## Future Improvements

### Potential Enhancements

1. **Worker threads**: Parallel processing for CPU-intensive operations
2. **Streaming inserts**: Direct streaming to database without intermediate storage
3. **Compression**: Compress geometry data before storage
4. **Caching**: Smart caching of frequently accessed data

### Metrics to Track

- Peak memory usage per dataset
- Processing time per 1000 features
- Database transaction success rates
- Memory growth rate during processing
